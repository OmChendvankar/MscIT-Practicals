22-June-2023
Practical :  Install Hadoop on Ubuntu(Basic)
Prerequisites : An Ubuntu server VM with a user having sudo privileges
===========================================
#check existing users on ubuntu
cut -d: -f1 /etc/passwd

# remove hadoop user
sudo deluser hduser

# ps aux | grep 
sudo killall -TERM -u hduser

## remove hadoop group
sudo deluser --group hadoop

# check presence of hadoop
# go to location  /usr/local/
# if you see a hadoop folder then hadoop installation was attempted and needs to be  removed before fresh installation

# remove hadoop
sudo rm -r -f /usr/local/hadoop/

==========================================

Step 1 — Installing Java
Step 2 — Installing Hadoop
Step 3 — Configuring Hadoop
Step 4 — Running Hadoop
===========================================================================

Step 1 — Installing Java
To get started, we’ll update our package list:
sudo apt update
 
Next, we’ll install OpenJDK, the default Java Development Kit on Ubuntu 18.04:
sudo apt install default-jdk

check folder for java installation at location - 
java path -/usr/lib/jvm/java-11-openjdk-amd64
Once the installation is complete, let’s check the version.
java -version
This output verifies that OpenJDK has been successfully installed.


===========================================================================
#Add newuser to new usergroup
#group-hadoop, #user - hduser
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser

#add new user to listed groups
sudo usermod -aG sudo hduser

#change to new user
su hduser
#The prompt shoud look like this - hduser@ubuntu:/
===========================================================================
#Create a ssh keygen for the user.
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys

# Disable IPv6.
sudo nano /etc/sysctl.conf

#add the following lines to the end of the file
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
===========================================================================
Step 2 — Downloading & Installing Hadoop
===========================================================================
#Extract Hadoop and move to group hadoop
cd /usr/local

sudo tar xvf /home/udit/Downloads/hadoop-3.2.3.tar.gz 
sudo mv hadoop-3.2.3 hadoop
sudo chown -R hduser: hadoop hadoop
===========================================================================
#Now open $HOME/.bashrc 
sudo nano $HOME/.bashrc

#add the following lines
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin

#Save file - Ctrl + s
#Close file - Ctrl + x

#Run the following command to make changes through the .bashrc file.
source ~/.bashrc

#Check version of java and hadoop
Command: java -version
Command: hadoop version
===========================================================================
===========================================================================
#Create a tmp folder in /app/hadoop/tmp and change the owner to hduser.
cd /usr/local
sudo mkdir -p /app/hadoop/tmp
sudo chown hduser:hadoop /app/hadoop/tmp/
===========================================================================
Step 3 — Configuring Hadoop
# Hadoop requires that you set the path to Java, either as an environment variable or in the Hadoop configuration file.hadoop-env.sh
cd /usr/local/hadoop/etc/hadoop/

#To Configure Hadoop’s Java Home, begin by opening hadoop-env.sh
sudo nano hadoop-env.sh

#Add the following line at the end of .sh file
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# save & close


#Make the changes in core-site.xml file
cd /usr/local/hadoop/etc/hadoop
sudo nano core-site.xml

#Add the following lines
<property>
	<name>hadoop.tmp.dir</name>
	<value>/app/hadoop/tmp</value>
	<description>A base for other temporarary directories</description>
</property>

<property>
	<name>fs.default.name</name>
	<value>hdfs://localhost:54310</value>
	<description>The name of the default file system.</description>
</property>

# save & Close
===========================================================================
#Make the changes in mapred-site.xml
sudo nano mapred-site.xml

#Add the following lines
<property>
	<name>mapred.job.tracker</name>
	<value>localhost:54311</value>
	<description>The host and port that the MapReduce job tracker runs
		at. If "local", then jobs are run in-process as a single map
		and reduce task.
	</description>
</property>
# save & Close
===========================================================================
#Make the changes in hdfs-site.xml
===========================================================================
sudo nano hdfs-site.xml

#Add the following lines
<property>
            <name>dfs.namenode.name.dir</name>
            <value>/app/hadoop/tmp/dfsdata/namenode</value>
    </property>

    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/app/hadoop/tmp/dfsdata/datanode</value>
    </property>

<property>
	<name>dfs.replication</name>
	<value>1</value>
	<description>Default block replication.
		     The actual number of replications can be specified when the file is 		     created. The default is used if replication is not specified in  			     create time.
	</description>
	
</property>

# save & Close
===========================================================================
#format namenode 
hdfs namenode -format
hdfs datanode -format
===========================================================================
# Step 4 - Running Hadoop
# To start hadoop, we need to start localhost
ssh localhost

# If connection is refused, it will result in error - run only if error & then run previous command
sudo apt-get install ssh

# Start all the hadoop services
/usr/local/hadoop/sbin/start-all.sh

# Check if that all hadoop services are running (6 services should appear)
jps

#Access localhost:9870 to get nanmenode status, open browser and type
http://localhost:9870

#Stop all the hadoop services.
/usr/local/hadoop/sbin/stop-all.sh
